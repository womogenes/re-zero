# OpenReasoning-Nemotron-14B â€” Network Log Analysis
# Qwen2ForCausalLM (standard transformer, 14.7B dense)
# 4x H100: 1 inference + 3 training (FSDP2 + LoRA)

inference_gpu_ids = [0]
trainer_gpu_ids = [1, 2, 3]

max_steps = 200
seq_len = 4096
output_dir = "/root/checkpoints/nemotron-network-logs"

[model]
name = "nvidia/OpenReasoning-Nemotron-14B"

[wandb]
project = "re-zero"
name = "nemotron-network-logs"
offline = true

[ckpt]
interval = 5

[trainer.model]
impl = "auto"

[trainer.model.lora]
rank = 32
alpha = 64
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

[trainer.tokenizer]
name = "nvidia/OpenReasoning-Nemotron-14B"

[trainer.optim]
lr = 1e-5

[orchestrator]
batch_size = 128
rollouts_per_example = 4

[orchestrator.sampling]
max_tokens = 1024

[[orchestrator.env]]
id = "intertwine/sv-env-network-logs"
name = "network-logs"

[inference]

[inference.model]
max_model_len = 65536

[inference.parallel]
dp = 1
