# GLM-4.7-Flash x Network Logs
# 4x H100: GPU 0 = inference (vLLM), GPUs 1-3 = trainer (FSDP)
# 30B MoE (3B active) with MLA compressed attention â€” needs LoRA

inference_gpu_ids = [0]
trainer_gpu_ids = [1, 2, 3]

max_steps = 50
seq_len = 4096
clean = true
output_dir = "/root/checkpoints/glm4flash-network-logs"

[model]
name = "zai-org/GLM-4.7-Flash"

[wandb]
project = "re-zero"
name = "glm4flash-network-logs"
offline = true

[ckpt]
interval = 5
keep_last = 3

[trainer.tokenizer]
name = "zai-org/GLM-4.7-Flash"
trust_remote_code = true

[trainer.model]
trust_remote_code = true

[trainer.model.lora]
rank = 16
alpha = 32.0
target_modules = ["q_b_proj", "kv_b_proj", "o_proj"]

[trainer.optim]
lr = 5e-6
weight_decay = 0.0

[orchestrator]
batch_size = 128
rollouts_per_example = 4

[orchestrator.sampling]
max_tokens = 512

[[orchestrator.env]]
id = "intertwine/sv-env-network-logs"

[inference]
gpu_memory_utilization = 0.95

[inference.model]
trust_remote_code = true
enable_auto_tool_choice = true
tool_call_parser = "hermes"
enforce_eager = true
max_model_len = 4096

[inference.parallel]
dp = 1
